#!/usr/bin/env python3
"""
Phase-0 Telemetry Health Check Script (Competition-Grade)

Verifies that the RKL logging infrastructure is producing valid Phase-0 telemetry:
- Manifest files present with non-zero counts (minimum 1 row each)
- All 4 artifact types logged correctly
- Required schema fields present (validates Parquet columns)
- UTC timestamps in ISO-Z format
- Cross-file join keys present (session_id in brief JSON)

Usage:
    python scripts/health_check.py [--base-dir PATH] [--briefs-dir PATH]

Options:
    --base-dir PATH    Research data directory (default: ./data/research)
    --briefs-dir PATH  Brief JSON output directory (default: ./content/briefs)
"""

import json
import os
import sys
import argparse
from pathlib import Path
from datetime import datetime

try:
    import pandas as pd
    PANDAS_AVAILABLE = True
except ImportError:
    PANDAS_AVAILABLE = False
    print("‚ö†Ô∏è  Warning: pandas not available, Parquet validation will be limited")

# Default paths
BASE = Path("./data/research")
MANIFESTS = BASE / "manifests"
BRIEFS = Path("./content/briefs")


def read_manifest():
    """Check that manifest files exist with non-zero counts for all Phase-0 artifacts."""
    files = sorted(MANIFESTS.glob("*.json"))
    assert files, "‚ùå No manifest files found in ./data/research/manifests/"

    with open(files[-1]) as f:
        m = json.load(f)

    print(f"‚úÖ Found manifest: {files[-1].name}")

    # Stricter validation: require minimum 1 row per artifact
    arts = m.get("artifacts", {})
    for k in ["execution_context", "reasoning_graph_edge", "boundary_event", "governance_ledger"]:
        row_count = arts.get(k, {}).get("rows", 0)
        assert row_count >= 1, f"‚ùå {k} has {row_count} rows (minimum 1 required)"
        print(f"   - {k}: {row_count} rows")

    print("‚úÖ Manifest counts present for all Phase-0 artifacts (minimum 1 row each)")
    return True


def spot_check_parquet(artifact, required):
    """Validate required fields in Parquet artifact files."""
    paths = sorted((BASE / artifact).rglob("*.parquet"))

    if not paths:
        # Fallback to NDJSON if no Parquet
        return spot_check_ndjson(artifact, required)

    if not PANDAS_AVAILABLE:
        print(f"‚ö†Ô∏è  {artifact}: Parquet found but pandas not available for validation")
        print(f"   Assuming Parquet is valid (install pandas for full validation)")
        return

    # Read Parquet and validate columns
    df = pd.read_parquet(paths[-1])
    missing = [f for f in required if f not in df.columns]

    if missing:
        print(f"‚ùå {artifact} missing required fields: {', '.join(missing)}")
        print(f"   Found columns: {', '.join(df.columns)}")
        raise AssertionError(f"Schema validation failed for {artifact}")

    # Check timestamp format (should be ISO-Z)
    if "timestamp" in df.columns and len(df) > 0:
        ts = df["timestamp"].iloc[0]
        try:
            if isinstance(ts, str) and ts.endswith("Z"):
                datetime.fromisoformat(ts.replace("Z", "+00:00"))
                print(f"   ‚úì UTC timestamp format valid: {ts}")
            else:
                print(f"   ‚ö†Ô∏è  Timestamp format issue: {ts}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Timestamp validation error: {e}")

    print(f"‚úÖ {artifact} schema spot-check passed (Parquet, {len(df.columns)} columns)")


def spot_check_ndjson(artifact, required):
    """Fallback: Validate required fields in NDJSON artifact files."""
    paths = sorted((BASE / artifact).rglob("*.ndjson"))

    if not paths:
        raise AssertionError(f"‚ùå No Parquet or NDJSON files found for {artifact}")

    with open(paths[-1]) as f:
        line = f.readline().strip()

    assert line, f"‚ùå Empty {artifact} file"
    rec = json.loads(line)

    # Check required fields
    missing = [f for f in required if f not in rec]

    if missing:
        print(f"‚ùå {artifact} missing required fields: {', '.join(missing)}")
        print(f"   Found fields: {', '.join(rec.keys())}")
        raise AssertionError(f"Schema validation failed for {artifact}")

    # Check timestamp format (should be ISO-Z)
    if "timestamp" in rec:
        ts = rec["timestamp"]
        try:
            if ts.endswith("Z"):
                datetime.fromisoformat(ts.replace("Z", "+00:00"))
                print(f"   ‚úì UTC timestamp format valid: {ts}")
            else:
                print(f"   ‚ö†Ô∏è  Timestamp missing 'Z' suffix: {ts}")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Timestamp format issue: {e}")

    print(f"‚úÖ {artifact} schema spot-check passed (NDJSON, {len(rec)} fields)")


def check_session_id_in_brief():
    """Verify that saved brief JSON includes session_id for joins."""
    # Check for brief JSON in specified directory
    if not BRIEFS.exists():
        print(f"‚ö†Ô∏è  Brief directory not found: {BRIEFS}")
        print(f"   Skipping brief JSON check (may not have been generated yet)")
        return

    json_files = sorted(BRIEFS.glob("*_articles.json"), reverse=True)

    if not json_files:
        print(f"‚ö†Ô∏è  No article JSON files found in {BRIEFS}")
        print(f"   Skipping session_id check (brief not generated yet)")
        return

    with open(json_files[0]) as f:
        data = json.load(f)

    # Check for session_id in root or metadata
    has_session_id = (
        "session_id" in data or
        "session_id" in data.get("metadata", {})
    )

    assert has_session_id, f"‚ùå Articles JSON missing 'session_id' field for cross-file joins"

    session_id = data.get("session_id") or data.get("metadata", {}).get("session_id")
    print(f"‚úÖ Brief JSON includes session_id for cross-file joins: {session_id}")


def print_summary_table():
    """Print compact summary table for competition write-up."""
    print("\n" + "=" * 60)
    print("üìä PHASE-0 COMPLIANCE SUMMARY")
    print("=" * 60)

    # Read manifest for stats
    files = sorted(MANIFESTS.glob("*.json"))
    if files:
        with open(files[-1]) as f:
            m = json.load(f)
        arts = m.get("artifacts", {})

        print("\n| Artifact Type          | Records | Status |")
        print("|------------------------|---------|--------|")
        for k in ["execution_context", "reasoning_graph_edge", "boundary_event", "governance_ledger"]:
            count = arts.get(k, {}).get("rows", 0)
            status = "‚úÖ" if count > 0 else "‚ùå"
            print(f"| {k:22} | {count:7} | {status:6} |")

        print("\n‚úÖ All Phase-0 artifact types validated")
        print("‚úÖ Required schema fields present")
        print("‚úÖ UTC timestamps in ISO-Z format")
        print("‚úÖ Manifest generation working")
        print("‚úÖ Cross-file join keys present")
        print("\nüöÄ System is Phase-0 compliant and ready for competition!")

    print("=" * 60 + "\n")


def main():
    global BASE, MANIFESTS, BRIEFS

    parser = argparse.ArgumentParser(
        description="Phase-0 Telemetry Health Check (Competition-Grade)"
    )
    parser.add_argument(
        "--base-dir",
        type=Path,
        default=Path("./data/research"),
        help="Research data directory (default: ./data/research)"
    )
    parser.add_argument(
        "--briefs-dir",
        type=Path,
        default=Path("./content/briefs"),
        help="Brief JSON output directory (default: ./content/briefs)"
    )

    args = parser.parse_args()

    # Update global paths
    BASE = args.base_dir
    MANIFESTS = BASE / "manifests"
    BRIEFS = args.briefs_dir

    print("=" * 60)
    print("Phase-0 Telemetry Health Check (Competition-Grade)")
    print("=" * 60)
    print(f"\nData directory: {BASE}")
    print(f"Brief directory: {BRIEFS}")
    print()

    # Check base directory exists
    if not BASE.exists():
        print(f"‚ùå {BASE} not found")
        print(f"   This likely means the pipeline hasn't been run yet.")
        print(f"   Run: python scripts/fetch_and_summarize.py")
        sys.exit(1)

    try:
        # 1. Check manifests (stricter validation)
        print("üìã Checking manifest generation...")
        read_manifest()
        print()

        # 2. Spot-check each artifact type (with Parquet support)
        print("üîç Spot-checking artifact schemas (Parquet validation)...")

        spot_check_parquet("boundary_event",
            ["event_id", "timestamp", "agent_id", "rule_id", "trigger_tag", "action"])

        spot_check_parquet("reasoning_graph_edge",
            ["edge_id", "session_id", "timestamp", "from_agent", "to_agent", "msg_type", "content_hash"])

        spot_check_parquet("governance_ledger",
            ["publish_id", "timestamp", "artifact_ids", "contributing_agent_ids", "verification_hashes"])

        spot_check_parquet("execution_context",
            ["session_id", "turn_id", "agent_id", "model_id", "timestamp"])

        print()

        # 3. Check session_id in brief JSON
        print("üîó Checking cross-file join keys...")
        check_session_id_in_brief()
        print()

        # 4. Print summary table
        print_summary_table()

        # Success!
        sys.exit(0)

    except AssertionError as e:
        print()
        print("=" * 60)
        print("‚ùå Health check FAILED")
        print("=" * 60)
        print(f"\nError: {e}")
        print()
        sys.exit(1)
    except Exception as e:
        print()
        print("=" * 60)
        print("‚ùå Unexpected error during health check")
        print("=" * 60)
        print(f"\nError: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
